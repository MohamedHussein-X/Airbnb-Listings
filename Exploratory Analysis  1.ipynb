{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msb\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "key_filepath = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "os.environ['PYSPARK_PYTHON'] = os.getenv(\"PYSPARK_PYTHON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[6]\") \\\n",
    "    .appName(\"Airbnb Listings\") \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\",key_filepath  ) \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .getOrCreate()       \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read data from GCS\n",
    "df = spark.read.parquet(\"gs://airbnb-listings-421017-bucket/airbnb_listings.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the listings based on room types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# get the number of listings per room type descending\n",
    "\n",
    "room_type_counts =df.groupBy(\"room_type\").count().orderBy(col(\"count\").desc()).toPandas()\n",
    "sb.barplot(x = \"room_type\",y=\"count\" ,  data = room_type_counts, palette=\"Set1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the listings based on the property type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "property_type_counts = df.groupBy(\"property_type\").count().orderBy(col(\"count\").desc()).toPandas()\n",
    "plt.figure(figsize=(22, 6))  # Adjust the width and height as needed\n",
    "sb.barplot(x = \"property_type\",y=\"count\" ,  data = property_type_counts, palette=\"Set1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the prices for the different room and property types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the price group by room_type and property_type\n",
    "price_group = df.groupBy(\"room_type\",\"property_type\").agg({\"price\":\"avg\"}).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_heatmap = price_group.pivot(index=\"property_type\", columns=\"room_type\", values=\"avg(price)\")\n",
    "# print(price_heatmap)\n",
    "plt.figure(figsize=(24, 12))  # Adjust the width and height as needed\n",
    "sb.heatmap(price_heatmap, cmap=\"Blues\", annot=True, fmt=\".0f\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anaylzing the listings based on the number of bedrooms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beds_count = df.select('price','bedrooms').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sb.boxplot(x='bedrooms', y='price', data=beds_count , palette=\"Set1\")\n",
    "plt.title('Boxplot of Average Price by Bedrooms')\n",
    "plt.xlabel('Bedrooms')\n",
    "plt.ylabel('Average Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing if any particular ammenity results in higher prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the top 100 amenities by price\n",
    "\n",
    "top_amenities =df.select('amenities').sort(col(\"price\").desc()).limit(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, regexp_extract, regexp_replace\n",
    "\n",
    "regex_pattern = r'[a-zA-Z]+' # extract only the words\n",
    "# remove the special characters\n",
    "top_amenities = top_amenities.withColumn(\"amenities\", regexp_replace(col(\"amenities\"), '[{}\"\\\\s]+', ' '))\n",
    "\n",
    "top_amenities = top_amenities.withColumn('amenities', explode(split(top_amenities['amenities'], ',')))\n",
    "\n",
    "# transform the data to lowercase and then return it as a list \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# flatten the column \n",
    "ammenities_list = top_amenities.select('amenities').rdd.flatMap(lambda x: x).collect() \n",
    "\n",
    "# apply trim and lower case to the data \n",
    "\n",
    "ammenities_list = [x.strip().lower() for x in ammenities_list]\n",
    "\n",
    "ammenities_words = ' '.join(ammenities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "wordcloud = WordCloud(width = 500, height = 400, background_color=\"white\").generate(ammenities_words)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
